{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'TTS.tts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-35305d229c3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TTS_repo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mTTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mTTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mTTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbols\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphonemes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'TTS.tts'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "import IPython\n",
    "\n",
    "# for some reason TTS installation does not work on Colab\n",
    "sys.path.append('TTS_repo')\n",
    "\n",
    "from TTS.tts.utils.generic_utils import setup_model\n",
    "from TTS.utils.io import load_config\n",
    "from TTS.tts.utils.text.symbols import symbols, phonemes\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.tts.utils.synthesis import synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/testchec/asistente\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile\t\tconfig_kusal.json    layers\t       tests\r\n",
      "LICENSE.txt\t\tdataset_analysis     models\t       train.py\r\n",
      "README.md\t\tdatasets\t     notebooks\t       utils\r\n",
      "TTS.egg-info\t\tdebug_config.py      requirements.txt  version.py\r\n",
      "ThisBranch.txt\t\textract_features.py  server\r\n",
      "best_model_config.json\thard-sentences.txt   setup.py\r\n",
      "config.json\t\timages\t\t     test_cluster.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/1402048/52643646-c2102980-2edd-11e9-8c37-b72f3c89a640.png\" data-canonical-src=\"![TTS banner](https://user-images.githubusercontent.com/1402048/52643646-c2102980-2edd-11e9-8c37-b72f3c89a640.png =250x250)\r\n",
      "\" width=\"320\" height=\"95\" /></p>\r\n",
      "\r\n",
      "This project is a part of [Mozilla Common Voice](https://voice.mozilla.org/en). TTS aims a deep learning based Text2Speech engine, low in cost and high in quality. To begin with, you can hear a sample generated voice from [here](https://soundcloud.com/user-565970875/commonvoice-loc-sens-attn).\r\n",
      "\r\n",
      "The model architecture is highly inspired by Tacotron: [A Fully End-to-End Text-To-Speech Synthesis Model](https://arxiv.org/abs/1703.10135). However, it has many important updates that make training faster and computationally very efficient. Feel free to experiment with new ideas and propose changes.\r\n",
      "\r\n",
      "You can find [here](http://www.erogol.com/text-speech-deep-learning-architectures/) a brief note about TTS architectures and their comparisons.\r\n",
      "\r\n",
      "## Requirements and Installation\r\n",
      "Highly recommended to use [miniconda](https://conda.io/miniconda.html) for easier installation.\r\n",
      "  * python>=3.6\r\n",
      "  * pytorch>=0.4.1\r\n",
      "  * librosa\r\n",
      "  * tensorboard\r\n",
      "  * tensorboardX\r\n",
      "  * matplotlib\r\n",
      "  * unidecode\r\n",
      "\r\n",
      "Install TTS using ```setup.py```. It will install all of the requirements automatically and make TTS available to all the python environment as an ordinary python module.\r\n",
      "\r\n",
      "```python setup.py develop```\r\n",
      "\r\n",
      "Or you can use ```requirements.txt``` to install the requirements only.\r\n",
      "\r\n",
      "```pip install -r requirements.txt```\r\n",
      "\r\n",
      "### Docker\r\n",
      "A barebone `Dockerfile` exists at the root of the project, which should let you quickly setup the environment. By default, it will start the server and let you query it. Make sure to use `nvidia-docker` to use your GPUs. Make sure you follow the instructions in the [`server README`](server/README.md) before you build your image so that the server can find the model within the image.\r\n",
      "\r\n",
      "```\r\n",
      "docker build -t mozilla-tts .\r\n",
      "nvidia-docker run -it --rm -p 5002:5002 mozilla-tts\r\n",
      "```\r\n",
      "\r\n",
      "## Checkpoints and Audio Samples\r\n",
      "Check out [here](https://mycroft.ai/blog/available-voices/#the-human-voice-is-the-most-perfect-instrument-of-all-arvo-part) to compare the samples (except the first) below.\r\n",
      "\r\n",
      "| Models        |Dataset | Commit            | Audio Sample  | Details |\r\n",
      "| ------------- |:------:|:-----------------:|:--------------|:--------|\r\n",
      "| [iter-62410](https://drive.google.com/open?id=1pjJNzENL3ZNps9n7k_ktGbpEl6YPIkcZ)|LJSpeech| [99d56f7](https://github.com/mozilla/TTS/tree/99d56f7e93ccd7567beb0af8fcbd4d24c48e59e9)           | [link](https://soundcloud.com/user-565970875/99d56f7-iter62410 )|First model with plain Tacotron implementation.|\r\n",
      "| [iter-170K](https://drive.google.com/open?id=16L6JbPXj6MSlNUxEStNn28GiSzi4fu1j) |LJSpeech| [e00bc66](https://github.com/mozilla/TTS/tree/e00bc66) |[link](https://soundcloud.com/user-565970875/april-13-2018-07-06pm-e00bc66-iter170k)|More stable and longer trained model.|\r\n",
      "| [iter-270K](https://drive.google.com/drive/folders/1Q6BKeEkZyxSGsocK2p_mqgzLwlNvbHFJ?usp=sharing)|LJSpeech|[256ed63](https://github.com/mozilla/TTS/tree/256ed63)|[link](https://soundcloud.com/user-565970875/sets/samples-1650226)|Stop-Token prediction is added, to detect end of speech.|\r\n",
      "| Best: [iter-120K](https://drive.google.com/open?id=1A5Hr6aSvfGgIiE20mBkpzyn3vvbR2APj) |LJSpeech| [bf7590](https://github.com/mozilla/TTS/tree/bf7590) | [link](https://soundcloud.com/user-565970875/sets/september-26-2018-bf7590) | Better for longer sentences |\r\n",
      "|NEW: [iter-108K](https://drive.google.com/open?id=1deQ2akq9cuyreda0DgZOiBdydkbgseWP)| TWEB | [2810d57](https://github.com/mozilla/TTS/tree/2810d57) | [link](https://soundcloud.com/user-565970875/tweb-example-108k-iters-2810d57) | https://github.com/mozilla/TTS/issues/22 | \r\n",
      "\r\n",
      "## Example Model Outputs\r\n",
      "Below you see model state after 16K iterations with batch-size 32.\r\n",
      "\r\n",
      "> \"Recent research at Harvard has shown meditating for as little as 8 weeks can actually increase the grey matter in the parts of the brain responsible for emotional regulation and learning.\"\r\n",
      "\r\n",
      "Audio output: [https://soundcloud.com/user-565970875/iter16k-f48c3b](https://soundcloud.com/user-565970875/iter16k-f48c3b)\r\n",
      "\r\n",
      "![example_model_output](images/example_model_output.png?raw=true)\r\n",
      "\r\n",
      "## Runtime\r\n",
      "The most time-consuming part is the vocoder algorithm (Griffin-Lim) which runs on CPU. By setting its number of iterations, you might have faster execution with a small loss of quality. Some of the experimental values are below.\r\n",
      "\r\n",
      "Sentence: \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\"\r\n",
      "\r\n",
      "Audio length is approximately 6 secs.\r\n",
      "\r\n",
      "| Time (secs) | System | # GL iters |\r\n",
      "| ---- |:-------|:-----------|\r\n",
      "|2.00|GTX1080Ti|30|\r\n",
      "|3.01|GTX1080Ti|60|\r\n",
      "\r\n",
      "\r\n",
      "## Datasets and Data-Loading\r\n",
      "TTS provides a generic dataloder easy to use for new datasets. You need to write an adaptor to format and that's all you need.Check ```datasets/preprocess.py``` to see example adaptors. After you wrote an adaptor, you need to set ```dataset``` field in ```config.json```. Do not forget other data related fields. \r\n",
      "\r\n",
      "You can also use pre-computed features. In this case, compute features with ```extract_features.py``` and set ```dataset``` field as ```tts_cache```. \r\n",
      "\r\n",
      "Example datasets, we successfully applied TTS, are linked below.\r\n",
      "\r\n",
      "- [LJ Speech](https://keithito.com/LJ-Speech-Dataset/)\r\n",
      "- [Nancy](http://www.cstr.ed.ac.uk/projects/blizzard/2011/lessac_blizzard2011/)\r\n",
      "- [TWEB](http://https://www.kaggle.com/bryanpark/the-world-english-bible-speech-dataset)\r\n",
      "- [M-AI-Labs](http://www.caito.de/2019/01/the-m-ailabs-speech-dataset/)\r\n",
      "\r\n",
      "## Training and Fine-tuning LJ-Speech\r\n",
      "[Click Here](https://gist.github.com/erogol/97516ad65b44dbddb8cd694953187c5b) for hands-on **Notebook example**, training LJSpeech.\r\n",
      "\r\n",
      "Split ```metadata.csv``` into train and validation subsets respectively ```metadata_train.csv``` and ```metadata_val.csv```. Note that having a validation split does not work well as oppose to other ML problems since at the validation time model generates spectrogram slices without \"Teacher-Forcing\" and that leads misalignment between the ground-truth and the prediction. Therefore, validation loss does not really show the model performance. Rather, you might use all data for training and check the model performance by relying on human inspection.\r\n",
      "\r\n",
      "```\r\n",
      "shuf metadata.csv > metadata_shuf.csv\r\n",
      "head -n 12000 metadata_shuf.csv > metadata_train.csv\r\n",
      "tail -n 1100 metadata_shuf.csv > metadata_val.csv\r\n",
      "```\r\n",
      "\r\n",
      "To train a new model, you need to define your own ```config.json``` file (check the example) and call with the command below.\r\n",
      "\r\n",
      "```train.py --config_path config.json```\r\n",
      "\r\n",
      "To fine-tune a model, use ```--restore_path```.\r\n",
      "\r\n",
      "```train.py --config_path config.json --restore_path /path/to/your/model.pth.tar```\r\n",
      "\r\n",
      "If you like to use a specific set of GPUs, you need to set an environment variable. The code uses automatically all the available GPUs for data parallel training. If you don't specify the GPUs, it uses them all.\r\n",
      "\r\n",
      "```CUDA_VISIBLE_DEVICES=\"0,1,4\" train.py --config_path config.json```\r\n",
      "\r\n",
      "Each run creates a new output folder and ```config.json``` is copied under this folder.\r\n",
      "\r\n",
      "In case of any error or intercepted execution, if there is no checkpoint yet under the output folder, the whole folder is going to be removed.\r\n",
      "\r\n",
      "You can also enjoy Tensorboard,  if you point the Tensorboard argument```--logdir``` to the experiment folder.\r\n",
      "\r\n",
      "## Testing\r\n",
      "Best way to test your network is to use Notebooks under ```notebooks``` folder.\r\n",
      "\r\n",
      "## What is new with TTS\r\n",
      "If you train TTS with LJSpeech dataset, you start to hear reasonable results after 12.5K iterations with batch size 32. This is the fastest training with character-based methods up to our knowledge. Out implementation is also quite robust against long sentences.\r\n",
      "\r\n",
      "- Location sensitive attention ([ref](https://arxiv.org/pdf/1506.07503.pdf)). Attention is a vital part of text2speech models. Therefore, it is important to use an attention mechanism that suits the diagonal nature of the problem where the output strictly aligns with the text monotonically. Location sensitive attention performs better by looking into the previous alignment vectors and learns diagonal attention more easily. Yet, I believe there is a good space for research at this front to find a better solution.\r\n",
      "- Attention smoothing with sigmoid ([ref](https://arxiv.org/pdf/1506.07503.pdf)). Attention weights are computed by normalized sigmoid values instead of softmax for sharper values. That enables the model to pick multiple highly scored inputs for alignments while reducing the noise.\r\n",
      "- Weight decay ([ref](http://www.fast.ai/2018/07/02/adam-weight-decay/)). After a certain point of the training, you might observe the model over-fitting. That is, the model is able to pronounce words probably better but the quality of the speech quality gets lower and sometimes attention alignment gets disoriented.\r\n",
      "- Stop token prediction with an additional module. The original Tacotron model does not propose a stop token to stop the decoding process. Therefore, you need to use heuristic measures to stop the decoder. Here, we prefer to use additional layers at the end to decide when to stop.\r\n",
      "- Applying sigmoid to the model outputs. Since the output values are expected to be in the range [0, 1], we apply sigmoid to make things easier to approximate the expected output distribution.\r\n",
      "- Phoneme based training is enabled for easier learning and robust pronunciation. It also makes easier to adapt TTS to the most languages without worrying about language specific characters.\r\n",
      "- Configurable attention windowing at inference-time for robust alignment. It enforces network to only consider a certain window of encoder steps per iteration.\r\n",
      "- Detailed Tensorboard stats for activation, weight and gradient values per layer. It is useful to detect defects and compare networks.\r\n",
      "- Constant history window. Instead of using only the last frame of predictions, define a constant history queue. It enables training with gradually decreasing prediction frame (r=5 --> r=1) by only changing the last layer. For instance, you can train the model with r=5 and then fine-tune it with r=1 without any performance loss. It also solves well-known PreNet problem [#50](https://github.com/mozilla/TTS/issues/50). \r\n",
      "- Initialization of hidden decoder states with Embedding layers instead of zero initialization. \r\n",
      "\r\n",
      "One common question is to ask why we don't use Tacotron2 architecture. According to our ablation experiments, nothing, except Location Sensitive Attention, improves the performance, given the increase in the model size.\r\n",
      "\r\n",
      "Please feel free to offer new changes and pull things off. We are happy to discuss and make things better.\r\n",
      "\r\n",
      "## Problems waiting to be solved.\r\n",
      "- Punctuations at the end of a sentence sometimes affect the pronunciation of the last word. Because punctuation sign is attended by the attention module, that forces the network to create a voice signal or at least modify the voice signal being generated for neighboring frames.\r\n",
      "- ~~Simpler stop-token prediction. Right now we use RNN to keep the history of the previous frames. However, we never tested, if something simpler would work as well.~~ Yet RNN based model gives more stable predictions.\r\n",
      "- Train for better mel-specs. Mel-spectrograms are not good enough to be fed Neural Vocoder. Easy solution to this problem is to train the model with r=1. However, in this case, model struggles to align the attention.\r\n",
      "- irregular words: \"minute\", \"focus\", \"aren't\" etc. Even though ~~it might be solved~~ (Use a better dataset like Nancy or train phonemes enabled.)\r\n",
      "\r\n",
      "## Major TODOs\r\n",
      "- [x] Implement the model.\r\n",
      "- [x] Generate human-like speech on LJSpeech dataset.\r\n",
      "- [x] Generate human-like speech on a different dataset (Nancy) (TWEB).\r\n",
      "- [x] Train TTS with r=1 successfully.\r\n",
      "- [ ] Enable process based distributed training. Similar [to] (https://github.com/fastai/imagenet-fast/).\r\n",
      "- [ ] Adapting Neural Vocoder. The most active work is [here] (https://github.com/erogol/WaveRNN)\r\n",
      "- [ ] Multi-speaker embedding.\r\n",
      "\r\n",
      "## References\r\n",
      "- [Efficient Neural Audio Synthesis](https://arxiv.org/pdf/1802.08435.pdf)\r\n",
      "- [Attention-Based models for speech recognition](https://arxiv.org/pdf/1506.07503.pdf)\r\n",
      "- [Generating Sequences With Recurrent Neural Networks](https://arxiv.org/pdf/1308.0850.pdf)\r\n",
      "- [Char2Wav: End-to-End Speech Synthesis](https://openreview.net/pdf?id=B1VWyySKx)\r\n",
      "- [VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop](https://arxiv.org/pdf/1707.06588.pdf)\r\n",
      "- [WaveRNN](https://arxiv.org/pdf/1802.08435.pdf)\r\n",
      "- [Faster WaveNet](https://arxiv.org/abs/1611.09482)\r\n",
      "- [Parallel WaveNet](https://arxiv.org/abs/1711.10433)\r\n",
      "\r\n",
      "### Precursor implementations\r\n",
      "- https://github.com/keithito/tacotron (Dataset and Test processing)\r\n",
      "- https://github.com/r9y9/tacotron_pytorch (Initial Tacotron architecture)\r\n"
     ]
    }
   ],
   "source": [
    "!cat TTS/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTSDataset.py  TTSDatasetMemory.py  __init__.py  preprocess.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls TTS/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\r\n",
      "import random\r\n",
      "\r\n",
      "def tts_cache(root_path, meta_file):\r\n",
      "    \"\"\"This format is set for the meta-file generated by extract_features.py\"\"\"\r\n",
      "    txt_file = os.path.join(root_path, meta_file)\r\n",
      "    items = []\r\n",
      "    with open(txt_file, 'r', encoding='utf8') as f:\r\n",
      "        for line in f:\r\n",
      "            cols = line.split('| ')\r\n",
      "            items.append(cols)  # text, wav_full_path, mel_name, linear_name, wav_len, mel_len\r\n",
      "    random.shuffle(items)\r\n",
      "    return items            \r\n",
      "\r\n",
      "\r\n",
      "def tweb(root_path, meta_file):\r\n",
      "    \"\"\"Normalize TWEB dataset. \r\n",
      "    https://www.kaggle.com/bryanpark/the-world-english-bible-speech-dataset\r\n",
      "    \"\"\"\r\n",
      "    txt_file = os.path.join(root_path, meta_file)\r\n",
      "    items = []\r\n",
      "    with open(txt_file, 'r') as ttf:\r\n",
      "        for line in ttf:\r\n",
      "            cols = line.split('\\t')\r\n",
      "            wav_file = os.path.join(root_path, cols[0]+'.wav')\r\n",
      "            text = cols[1]\r\n",
      "            items.append([text, wav_file])\r\n",
      "    random.shuffle(items)\r\n",
      "    return items\r\n",
      "    \r\n",
      "\r\n",
      "# def kusal(root_path, meta_file):\r\n",
      "#     txt_file = os.path.join(root_path, meta_file)\r\n",
      "#     texts = []\r\n",
      "#     wavs = []\r\n",
      "#     with open(txt_file, \"r\", encoding=\"utf8\") as f:\r\n",
      "#         frames = [\r\n",
      "#             line.split('\\t') for line in f\r\n",
      "#             if line.split('\\t')[0] in self.wav_files_dict.keys()\r\n",
      "#         ]\r\n",
      "#     # TODO: code the rest\r\n",
      "#     return  {'text': texts, 'wavs': wavs}\r\n",
      "\r\n",
      "\r\n",
      "def mailabs(root_path, meta_files):\r\n",
      "        \"\"\"Normalizes M-AI-Labs meta data files to TTS format\"\"\"\r\n",
      "        folders = [os.path.dirname(f.strip()) for f in meta_files.split(\",\")]\r\n",
      "        meta_files = [f.strip() for f in meta_files.split(\",\")]\r\n",
      "        items = []\r\n",
      "        for idx, meta_file in enumerate(meta_files):\r\n",
      "                print(\" | > {}\".format(meta_file))\r\n",
      "                folder = folders[idx]\r\n",
      "                txt_file = os.path.join(root_path, meta_file)\r\n",
      "                with open(txt_file, 'r') as ttf:\r\n",
      "                        for line in ttf:\r\n",
      "                                cols = line.split('|')\r\n",
      "                                wav_file = os.path.join(root_path, folder, 'wavs', cols[0]+'.wav')\r\n",
      "                                if os.path.isfile(wav_file):\r\n",
      "                                        text = cols[1]\r\n",
      "                                        items.append([text, wav_file])\r\n",
      "                                else: \r\n",
      "                                        continue\r\n",
      "        random.shuffle(items)\r\n",
      "        return items\r\n",
      "\r\n",
      "\r\n",
      "def ljspeech(root_path, meta_file):\r\n",
      "    \"\"\"Normalizes the Nancy meta data file to TTS format\"\"\"\r\n",
      "    txt_file = os.path.join(root_path, meta_file)\r\n",
      "    items = []\r\n",
      "    with open(txt_file, 'r') as ttf:\r\n",
      "        for line in ttf:\r\n",
      "            cols = line.split('|')\r\n",
      "            wav_file = os.path.join(root_path, 'wavs', cols[0]+'.wav')\r\n",
      "            text = cols[1]\r\n",
      "            items.append([text, wav_file])\r\n",
      "    random.shuffle(items)\r\n",
      "    return items\r\n",
      "\r\n",
      "\r\n",
      "def nancy(root_path, meta_file):\r\n",
      "    \"\"\"Normalizes the Nancy meta data file to TTS format\"\"\"\r\n",
      "    txt_file = os.path.join(root_path, meta_file)\r\n",
      "    items = []\r\n",
      "    with open(txt_file, 'r') as ttf:\r\n",
      "        for line in ttf:\r\n",
      "            id = line.split()[1]\r\n",
      "            text = line[line.find('\"')+1:line.rfind('\"')-1]\r\n",
      "            wav_file = root_path + 'wavn/' + id + '.wav'\r\n",
      "            items.append([text, wav_file])\r\n",
      "    random.shuffle(items)    \r\n",
      "    return items"
     ]
    }
   ],
   "source": [
    "!cat TTS/datasets/preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__doc__', '__loader__', '__name__', '__package__', '__path__', '__spec__']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(TTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "\"github_branch\":\"* dev\",\r\n",
      "\"restore_path\":\"/home/erogol/Models/LJSpeech/mai-es-ddc-September-23-2020_07+34PM-72dacd0/checkpoint_270000.pth.tar\",\r\n",
      "\"github_branch\":\"* differential-spec-loss\",\r\n",
      "\"restore_path\":\"/home/erogol/Models/LJSpeech/ljspeech-ddc-September-22-2020_03+25PM-72dacd0/checkpoint_50000.pth.tar\",\r\n",
      "    \"model\": \"Tacotron2\",\r\n",
      "    \"run_name\": \"mai-es-ddc\",\r\n",
      "    \"run_description\": \"tacotron2 with DDC and differential spectral loss with M-AI labs ES dataset (angelina)\",\r\n",
      "\r\n",
      "    // AUDIO PARAMETERS\r\n",
      "    \"audio\":{\r\n",
      "        // stft parameters\r\n",
      "        \"fft_size\": 1024,         // number of stft frequency levels. Size of the linear spectogram frame.\r\n",
      "        \"win_length\": 1024,      // stft window length in ms.\r\n",
      "        \"hop_length\": 256,       // stft window hop-lengh in ms.\r\n",
      "        \"frame_length_ms\": null, // stft window length in ms.If null, 'win_length' is used.\r\n",
      "        \"frame_shift_ms\": null,  // stft window hop-lengh in ms. If null, 'hop_length' is used.\r\n",
      "\r\n",
      "        // Audio processing parameters\r\n",
      "        \"sample_rate\": 16000,   // DATASET-RELATED: wav sample-rate.\r\n",
      "        \"preemphasis\": 0.0,     // pre-emphasis to reduce spec noise and make it more structured. If 0.0, no -pre-emphasis.\r\n",
      "        \"ref_level_db\": 20,     // reference level db, theoretically 20db is the sound of air.\r\n",
      "\r\n",
      "        // Silence trimming\r\n",
      "        \"do_trim_silence\": true,// enable trimming of slience of audio as you load it. LJspeech (true), TWEB (false), Nancy (true)\r\n",
      "        \"trim_db\": 60,          // threshold for timming silence. Set this according to your dataset.\r\n",
      "\r\n",
      "        // Griffin-Lim\r\n",
      "        \"power\": 1.5,           // value to sharpen wav signals after GL algorithm.\r\n",
      "        \"griffin_lim_iters\": 60,// #griffin-lim iterations. 30-60 is a good range. Larger the value, slower the generation.\r\n",
      "\r\n",
      "        // MelSpectrogram parameters\r\n",
      "        \"num_mels\": 80,         // size of the mel spec frame.\r\n",
      "        \"mel_fmin\": 50.0,        // minimum freq level for mel-spec. ~50 for male and ~95 for female voices. Tune for dataset!!\r\n",
      "        \"mel_fmax\": 7600.0,     // maximum freq level for mel-spec. Tune for dataset!!\r\n",
      "        \"spec_gain\": 1,\r\n",
      "\r\n",
      "        // Normalization parameters\r\n",
      "        \"signal_norm\": true,    // normalize spec values. Mean-Var normalization if 'stats_path' is defined otherwise range normalization defined by the other params.\r\n",
      "        \"min_level_db\": -100,   // lower bound for normalization\r\n",
      "        \"symmetric_norm\": true, // move normalization to range [-1, 1]\r\n",
      "        \"max_norm\": 4.0,        // scale normalization to range [-max_norm, max_norm] or [0, max_norm]\r\n",
      "        \"clip_norm\": true,      // clip normalized values into the range.\r\n",
      "        \"stats_path\": \"./scale_stats.npy\"    // DO NOT USE WITH MULTI_SPEAKER MODEL. scaler stats file computed by 'compute_statistics.py'. If it is defined, mean-std based notmalization is used and other normalization params are ignored\r\n",
      "    },\r\n",
      "\r\n",
      "    // VOCABULARY PARAMETERS\r\n",
      "    // if custom character set is not defined,\r\n",
      "    // default set in symbols.py is used\r\n",
      "      \"characters\":{\r\n",
      "             \"pad\": \"_\",\r\n",
      "             \"eos\": \"~\",\r\n",
      "             \"bos\": \"^\",\r\n",
      "             \"characters\": \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!'(),-.:;? \",\r\n",
      "             \"punctuations\":\"!'(),-.:;? \",\r\n",
      "             \"phonemes\":\"abcdefhijklmnopqrstuvwxyzæçðøħŋœǀǁǂǃɐɑɒɓɔɕɖɗɘəɚɛɜɞɟɠɡɢɣɤɥɦɧɨɪɫɬɭɮɯɰɱɲɳɴɵɶɸɹɺɻɽɾʀʁʂʃʄʈʉʊʋʌʍʎʏʐʑʒʔʕʘʙʛʜʝʟʡʢˈˌːˑ˞βθχᵻⱱ\"\r\n",
      "        },\r\n",
      "\r\n",
      "    // DISTRIBUTED TRAINING\r\n",
      "    \"distributed\":{\r\n",
      "        \"backend\": \"nccl\",\r\n",
      "        \"url\": \"tcp:\\/\\/localhost:54321\"\r\n",
      "    },\r\n",
      "\r\n",
      "    \"reinit_layers\": [],    // give a list of layer names to restore from the given checkpoint. If not defined, it reloads all heuristically matching layers.\r\n",
      "\r\n",
      "    // TRAINING\r\n",
      "    \"batch_size\": 32,       // Batch size for training. Lower values than 32 might cause hard to learn attention. It is overwritten by 'gradual_training'.\r\n",
      "    \"eval_batch_size\":16,\r\n",
      "    \"r\": 7,                 // Number of decoder frames to predict per iteration. Set the initial values if gradual training is enabled.\r\n",
      "    \"gradual_training\": [[0, 7, 64], [1, 5, 64], [50000, 3, 32], [130000, 2, 32], [290000, 1, 32]], //set gradual training steps [first_step, r, batch_size]. If it is null, gradual training is disabled. For Tacotron, you might need to reduce the 'batch_size' as you proceeed.\r\n",
      "    \"apex_amp_level\": null,     // level of optimization with NVIDIA's apex feature for automatic mixed FP16/FP32 precision (AMP), NOTE: currently only O1 is supported, and use \"O1\" to activate.\r\n",
      "\r\n",
      "    // LOSS SETTINGS\r\n",
      "    \"loss_masking\": true,       // enable / disable loss masking against the sequence padding.\r\n",
      "    \"decoder_loss_alpha\": 0.5,  // decoder loss weight. If > 0, it is enabled\r\n",
      "    \"postnet_loss_alpha\": 0.25, // postnet loss weight. If > 0, it is enabled\r\n",
      "    \"ga_alpha\": 5.0,           // weight for guided attention loss. If > 0, guided attention is enabled.\r\n",
      "    \"diff_spec_alpha\": 0.25,     // differential spectral loss weight. If > 0, it is enabled\r\n",
      "\r\n",
      "    // VALIDATION\r\n",
      "    \"run_eval\": true,\r\n",
      "    \"test_delay_epochs\": 10,  //Until attention is aligned, testing only wastes computation time.\r\n",
      "    \"test_sentences_file\": null,  // set a file to load sentences to be used for testing. If it is null then we use default english sentences.\r\n",
      "\r\n",
      "    // OPTIMIZER\r\n",
      "    \"noam_schedule\": false,        // use noam warmup and lr schedule.\r\n",
      "    \"grad_clip\": 1.0,              // upper limit for gradients for clipping.\r\n",
      "    \"epochs\": 2000,                // total number of epochs to train.\r\n",
      "    \"lr\": 0.0001,                  // Initial learning rate. If Noam decay is active, maximum learning rate.\r\n",
      "    \"wd\": 0.000001,                // Weight decay weight.\r\n",
      "    \"warmup_steps\": 4000,          // Noam decay steps to increase the learning rate from 0 to \"lr\"\r\n",
      "    \"seq_len_norm\": false,         // Normalize eash sample loss with its length to alleviate imbalanced datasets. Use it if your dataset is small or has skewed distribution of sequence lengths.\r\n",
      "\r\n",
      "    // TACOTRON PRENET\r\n",
      "    \"memory_size\": -1,             // ONLY TACOTRON - size of the memory queue used fro storing last decoder predictions for auto-regression. If < 0, memory queue is disabled and decoder only uses the last prediction frame.\r\n",
      "    \"prenet_type\": \"original\",     // \"original\" or \"bn\".\r\n",
      "    \"prenet_dropout\": false,       // enable/disable dropout at prenet.\r\n",
      "\r\n",
      "    // TACOTRON ATTENTION\r\n",
      "    \"attention_type\": \"original\",  // 'original' or 'graves'\r\n",
      "    \"attention_heads\": 4,          // number of attention heads (only for 'graves')\r\n",
      "    \"attention_norm\": \"sigmoid\",   // softmax or sigmoid.\r\n",
      "    \"windowing\": false,            // Enables attention windowing. Used only in eval mode.\r\n",
      "    \"use_forward_attn\": false,     // if it uses forward attention. In general, it aligns faster.\r\n",
      "    \"forward_attn_mask\": false,    // Additional masking forcing monotonicity only in eval mode.\r\n",
      "    \"transition_agent\": false,     // enable/disable transition agent of forward attention.\r\n",
      "    \"location_attn\": true,         // enable_disable location sensitive attention. It is enabled for TACOTRON by default.\r\n",
      "    \"bidirectional_decoder\": false,  // use https://arxiv.org/abs/1907.09006. Use it, if attention does not work well with your dataset.\r\n",
      "    \"double_decoder_consistency\": true,  // use DDC explained here https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency-draft/\r\n",
      "    \"ddc_r\": 7,                           // reduction rate for coarse decoder.\r\n",
      "\r\n",
      "    // STOPNET\r\n",
      "    \"stopnet\": true,               // Train stopnet predicting the end of synthesis.\r\n",
      "    \"separate_stopnet\": true,      // Train stopnet seperately if 'stopnet==true'. It prevents stopnet loss to influence the rest of the model. It causes a better model, but it trains SLOWER.\r\n",
      "\r\n",
      "    // TENSORBOARD and LOGGING\r\n",
      "    \"print_step\": 25,       // Number of steps to log training on console.\r\n",
      "    \"tb_plot_step\": 100,    // Number of steps to plot TB training figures.\r\n",
      "    \"print_eval\": false,     // If True, it prints intermediate loss values in evalulation.\r\n",
      "    \"save_step\": 10000,      // Number of training steps expected to save traninpg stats and checkpoints.\r\n",
      "    \"checkpoint\": true,     // If true, it saves checkpoints per \"save_step\"\r\n",
      "    \"tb_model_param_stats\": false,     // true, plots param stats per layer on tensorboard. Might be memory consuming, but good for debugging.\r\n",
      "\r\n",
      "    // DATA LOADING\r\n",
      "    \"text_cleaner\": \"phoneme_cleaners\",\r\n",
      "    \"enable_eos_bos_chars\": false, // enable/disable beginning of sentence and end of sentence chars.\r\n",
      "    \"num_loader_workers\": 4,        // number of training data loader processes. Don't set it too big. 4-8 are good values.\r\n",
      "    \"num_val_loader_workers\": 4,    // number of evaluation data loader processes.\r\n",
      "    \"batch_group_size\": 4,  //Number of batches to shuffle after bucketing.\r\n",
      "    \"min_seq_len\": 6,       // DATASET-RELATED: minimum text length to use in training\r\n",
      "    \"max_seq_len\": 153,     // DATASET-RELATED: maximum text length\r\n",
      "\r\n",
      "    // PATHS\r\n",
      "    \"output_path\": \"/home/erogol/Models/LJSpeech/\",\r\n",
      "\r\n",
      "    // PHONEMES\r\n",
      "    \"phoneme_cache_path\": \"/home/erogol/Models/phoneme_cache/\",  // phoneme computation is slow, therefore, it caches results in the given folder.\r\n",
      "    \"use_phonemes\": true,           // use phonemes instead of raw characters. It is suggested for better pronounciation.\r\n",
      "    \"phoneme_language\": \"es\",     // depending on your target language, pick one from  https://github.com/bootphon/phonemizer#languages\r\n",
      "\r\n",
      "    // MULTI-SPEAKER and GST\r\n",
      "    \"use_speaker_embedding\": false,      // use speaker embedding to enable multi-speaker learning.\r\n",
      "    \"use_gst\": false,       \t\t\t    // use global style tokens\r\n",
      "    \"use_external_speaker_embedding_file\": false, // if true, forces the model to use external embedding per sample instead of nn.embeddings, that is, it supports external embeddings such as those used at: https://arxiv.org/abs /1806.04558\r\n",
      "    \"external_speaker_embedding_file\": \"../../speakers-vctk-en.json\", // if not null and use_external_speaker_embedding_file is true, it is used to load a specific embedding file and thus uses these embeddings instead of nn.embeddings, that is, it supports external embeddings such as those used at: https://arxiv.org/abs /1806.04558\r\n",
      "    \"gst\":\t{\t\t\t                // gst parameter if gst is enabled\r\n",
      "        \"gst_use_speaker_embedding\": false,\r\n",
      "        \"gst_style_input\": null,        // Condition the style input either on a\r\n",
      "                                        // -> wave file [path to wave] or\r\n",
      "                                        // -> dictionary using the style tokens {'token1': 'value', 'token2': 'value'} example {\"0\": 0.15, \"1\": 0.15, \"5\": -0.15}\r\n",
      "                                        // with the dictionary being len(dict) <= len(gst_style_tokens).\r\n",
      "        \"gst_embedding_dim\": 512,\r\n",
      "        \"gst_num_heads\": 4,\r\n",
      "        \"gst_style_tokens\": 10\r\n",
      "\t},\r\n",
      "\r\n",
      "    // DATASETS\r\n",
      "    \"datasets\":   // List of datasets. They all merged and they get different speaker_ids.\r\n",
      "        [\r\n",
      "            {\r\n",
      "                \"name\": \"ljspeech\",\r\n",
      "                \"path\": \"/home/erogol/Data/es_ES/by_book/female/karen_savage/angelina/\",\r\n",
      "                \"meta_file_train\": \"metadata.csv\", // for vtck if list, ignore speakers id in list for train, its useful for test cloning with new speakers\r\n",
      "                \"meta_file_val\": null\r\n",
      "            }\r\n",
      "        ]\r\n",
      "}\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat tts_model/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
